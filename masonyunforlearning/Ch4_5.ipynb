{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 -> 기울기 산출 -> 매개변수 갱신 -> 1 ~ 3 반복\n",
    "확률적 경사 하강법(SGD) : 데이터를 미니배치로 무작위로 선정하기 때문 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two layered Network\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "    def predict(self, x): \n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    # x : 입력데이터, t : 정답 테이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t)\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09489868 0.09697798 0.10029724 0.09822704 0.09715464 0.10368148\n",
      "  0.08882073 0.1078613  0.10895127 0.10312963]\n",
      " [0.09531687 0.09743861 0.09997399 0.09850761 0.09690068 0.10386824\n",
      "  0.08850936 0.10739063 0.10927418 0.10281984]\n",
      " [0.09525205 0.09705974 0.09988768 0.09859943 0.09680277 0.10385995\n",
      "  0.08853446 0.10743324 0.10928028 0.1032904 ]\n",
      " [0.09483393 0.09749759 0.10044797 0.09845506 0.09691886 0.10355617\n",
      "  0.08875947 0.10736737 0.10936712 0.10279645]\n",
      " [0.09464508 0.09732079 0.09998164 0.09836474 0.09686215 0.10406968\n",
      "  0.08871398 0.10794758 0.10909325 0.10300111]\n",
      " [0.09500964 0.09753153 0.09990591 0.09840901 0.09683623 0.10386991\n",
      "  0.08844827 0.10762909 0.10908128 0.10327912]\n",
      " [0.09491363 0.09705274 0.10024125 0.09842547 0.09678245 0.10384275\n",
      "  0.08870405 0.10747877 0.10915749 0.10340139]\n",
      " [0.09472161 0.09740547 0.10017151 0.09834663 0.09675758 0.10368572\n",
      "  0.08891552 0.1075404  0.10894979 0.10350576]\n",
      " [0.09498057 0.09708686 0.10016077 0.09856449 0.09707899 0.1035795\n",
      "  0.08843964 0.10778393 0.10906167 0.10326358]\n",
      " [0.09499283 0.09723705 0.09980508 0.09836716 0.09677175 0.10351575\n",
      "  0.08885938 0.10760332 0.10926024 0.10358742]\n",
      " [0.09448662 0.09749265 0.10009261 0.09846293 0.09655286 0.10406125\n",
      "  0.08853712 0.10783156 0.10908019 0.1034022 ]\n",
      " [0.09502247 0.09706887 0.10019093 0.09860448 0.0966088  0.10394476\n",
      "  0.08868108 0.10747508 0.10924762 0.10315592]\n",
      " [0.09499058 0.09729446 0.09989116 0.0983889  0.09709927 0.10384567\n",
      "  0.08852762 0.10777309 0.10893323 0.10325603]\n",
      " [0.09442559 0.09731536 0.10023937 0.09832574 0.09682031 0.10403562\n",
      "  0.08891283 0.1074721  0.10935365 0.10309944]\n",
      " [0.09497372 0.09727424 0.09996506 0.09860567 0.09722951 0.1033796\n",
      "  0.08883941 0.1075241  0.10866747 0.10354123]\n",
      " [0.09480721 0.09760659 0.0998418  0.09825218 0.09701001 0.10353204\n",
      "  0.08901951 0.1078628  0.10916723 0.10290062]\n",
      " [0.09440349 0.09750345 0.09993929 0.0986045  0.09704586 0.10357245\n",
      "  0.08871612 0.10759661 0.1092526  0.10336563]\n",
      " [0.09512355 0.09735018 0.10012662 0.09850849 0.09694037 0.10342845\n",
      "  0.08878883 0.10752269 0.1091389  0.10307192]\n",
      " [0.09493238 0.09724074 0.10033228 0.09837158 0.09694873 0.10355968\n",
      "  0.08876386 0.10734106 0.10919766 0.10331203]\n",
      " [0.09463554 0.09717164 0.10030201 0.09834675 0.09692294 0.10382677\n",
      "  0.08904261 0.1076543  0.10861492 0.10348252]\n",
      " [0.09490232 0.09737006 0.10021029 0.09845623 0.09689587 0.10396761\n",
      "  0.08865187 0.10764023 0.10893958 0.10296595]\n",
      " [0.09504433 0.09749417 0.10033141 0.09826193 0.096376   0.10368183\n",
      "  0.08900502 0.10762366 0.10927366 0.102908  ]\n",
      " [0.09517803 0.09701126 0.09993331 0.09836171 0.09700617 0.10367277\n",
      "  0.08888464 0.1074562  0.10915347 0.10334244]\n",
      " [0.09481675 0.09735594 0.09992398 0.098653   0.09657091 0.10382224\n",
      "  0.08853294 0.10750313 0.1092506  0.10357051]\n",
      " [0.09472876 0.09744852 0.09981514 0.09854796 0.0970974  0.10356381\n",
      "  0.08903798 0.10727666 0.10933977 0.10314401]\n",
      " [0.09457704 0.09743389 0.10031734 0.09866278 0.09670309 0.10383771\n",
      "  0.08877122 0.10733128 0.10923657 0.10312908]\n",
      " [0.09504505 0.09749605 0.09991848 0.0984575  0.09684585 0.10375489\n",
      "  0.08836657 0.10773285 0.10927345 0.10310931]\n",
      " [0.09494252 0.09728536 0.09986889 0.09837754 0.09682658 0.10403659\n",
      "  0.0886378  0.1074705  0.10974777 0.10280644]\n",
      " [0.09515979 0.09723446 0.09970781 0.09841767 0.09694124 0.10381566\n",
      "  0.08898362 0.10741747 0.10901801 0.10330428]\n",
      " [0.09479006 0.09738142 0.09989621 0.09849315 0.09674923 0.10385295\n",
      "  0.08923659 0.10767903 0.10920016 0.1027212 ]\n",
      " [0.0951925  0.0969962  0.10031193 0.09831321 0.09696964 0.10422184\n",
      "  0.08867462 0.1071582  0.10932549 0.10283638]\n",
      " [0.09469499 0.09750176 0.10031609 0.09878903 0.09667915 0.10370507\n",
      "  0.08848508 0.10767608 0.10882156 0.1033312 ]\n",
      " [0.09493854 0.09734404 0.09996866 0.0982736  0.09700327 0.10381133\n",
      "  0.08872101 0.10764383 0.10887434 0.10342136]\n",
      " [0.09511836 0.0975863  0.09997981 0.09852582 0.09681728 0.10384082\n",
      "  0.08834359 0.10741551 0.10912223 0.10325027]\n",
      " [0.09460545 0.09737055 0.10000859 0.09826574 0.09713066 0.10390695\n",
      "  0.08869254 0.10770851 0.10901249 0.10329851]\n",
      " [0.09502167 0.09748037 0.09976777 0.09846016 0.09694232 0.10382515\n",
      "  0.0887564  0.10728742 0.1091219  0.10333681]\n",
      " [0.09471052 0.09731235 0.09992944 0.09862449 0.09694281 0.10381948\n",
      "  0.08877376 0.10746152 0.10926801 0.10315763]\n",
      " [0.09501216 0.09745193 0.0999538  0.09847875 0.09696527 0.10337865\n",
      "  0.08883335 0.10769472 0.1091246  0.10310677]\n",
      " [0.0949172  0.09729162 0.10019142 0.09860755 0.09684234 0.10376564\n",
      "  0.08866632 0.10749944 0.10904293 0.10317554]\n",
      " [0.09517915 0.09758271 0.10008144 0.09838526 0.09684834 0.10385639\n",
      "  0.08876131 0.10751153 0.10891848 0.1028754 ]\n",
      " [0.09487011 0.0973604  0.10011166 0.09891994 0.0968957  0.10338554\n",
      "  0.08845807 0.10749697 0.10902158 0.10348004]\n",
      " [0.09480444 0.09724331 0.10004664 0.09848094 0.09720482 0.10361583\n",
      "  0.08879287 0.10743695 0.10904756 0.10332663]\n",
      " [0.09544348 0.09727213 0.09981512 0.0987361  0.0969438  0.10354176\n",
      "  0.08871709 0.107334   0.10908598 0.10311054]\n",
      " [0.09483325 0.09767552 0.10009542 0.0984159  0.09677103 0.10359136\n",
      "  0.08897846 0.10764525 0.10860896 0.10338484]\n",
      " [0.09513998 0.09716229 0.09993007 0.09825037 0.09693178 0.10366633\n",
      "  0.08880473 0.10774862 0.10919027 0.10317557]\n",
      " [0.09519743 0.09694295 0.10017071 0.09838537 0.09689939 0.1036375\n",
      "  0.08854237 0.10784599 0.10903235 0.10334594]\n",
      " [0.09477807 0.09742891 0.1000263  0.0985285  0.09687342 0.10403405\n",
      "  0.08905813 0.10731427 0.10922978 0.10272858]\n",
      " [0.09497881 0.09693021 0.10018755 0.09872617 0.09657704 0.10397434\n",
      "  0.08858437 0.10766032 0.10901878 0.1033624 ]\n",
      " [0.09464751 0.09724316 0.10024089 0.09857376 0.09707969 0.10354271\n",
      "  0.08867113 0.10746886 0.10934874 0.10318354]\n",
      " [0.09505162 0.09742413 0.09979367 0.09866126 0.09696681 0.10385212\n",
      "  0.08835847 0.10739868 0.1089671  0.10352614]\n",
      " [0.09486368 0.09729276 0.10028798 0.09823963 0.09668487 0.10403509\n",
      "  0.08859445 0.10749821 0.10921531 0.10328803]\n",
      " [0.09496976 0.0969076  0.09979061 0.09857457 0.09700828 0.10390056\n",
      "  0.08867873 0.10740564 0.10953216 0.1032321 ]\n",
      " [0.09476965 0.09720157 0.10012777 0.09849513 0.09714737 0.10410867\n",
      "  0.08850787 0.10751916 0.10916346 0.10295933]\n",
      " [0.0947861  0.09729761 0.09976035 0.09833812 0.09696483 0.10368454\n",
      "  0.0885747  0.1077566  0.10935013 0.10348702]\n",
      " [0.09501103 0.09750896 0.09987951 0.09849665 0.09682634 0.10384922\n",
      "  0.08852359 0.10763938 0.10903632 0.10322901]\n",
      " [0.09495474 0.09731582 0.09980039 0.09870253 0.09688228 0.10385669\n",
      "  0.08897725 0.1074398  0.10896355 0.10310694]\n",
      " [0.09516258 0.09740742 0.10004307 0.09879522 0.09676627 0.10349414\n",
      "  0.08896938 0.10739211 0.10889568 0.10307413]\n",
      " [0.09513982 0.09712714 0.10010106 0.09839012 0.09677823 0.10406428\n",
      "  0.08874353 0.10747187 0.10884366 0.10334029]\n",
      " [0.09488823 0.0973262  0.10017073 0.09851683 0.09697511 0.10389188\n",
      "  0.08850367 0.10769985 0.10894735 0.10308015]\n",
      " [0.09452195 0.09752617 0.10009483 0.0983828  0.09684914 0.10346196\n",
      "  0.08881376 0.10773142 0.10940685 0.10321112]\n",
      " [0.0948535  0.09726737 0.10040931 0.09854541 0.09716522 0.10382099\n",
      "  0.08851735 0.10746637 0.1092232  0.1027313 ]\n",
      " [0.09506523 0.0971628  0.09998479 0.09868783 0.09717234 0.10399651\n",
      "  0.08853063 0.10725398 0.10891079 0.10323511]\n",
      " [0.09486175 0.09714697 0.10013938 0.09836124 0.09678641 0.10345251\n",
      "  0.08894594 0.10760434 0.10913306 0.1035684 ]\n",
      " [0.09471339 0.09735509 0.10030376 0.0985995  0.09663477 0.10363346\n",
      "  0.08852834 0.1079703  0.109061   0.10320038]\n",
      " [0.09481175 0.09730863 0.09987155 0.09857514 0.09670615 0.10376422\n",
      "  0.08909034 0.10749014 0.10902735 0.10335472]\n",
      " [0.09463493 0.09699388 0.10010303 0.09881562 0.09680706 0.10380607\n",
      "  0.08878551 0.10760871 0.10870366 0.10374155]\n",
      " [0.09472545 0.09744099 0.10010686 0.09836077 0.09681456 0.10383936\n",
      "  0.08864706 0.1074659  0.10938894 0.1032101 ]\n",
      " [0.09481717 0.09721414 0.09994828 0.09832552 0.09676017 0.10374072\n",
      "  0.08902319 0.10768694 0.10907976 0.10340411]\n",
      " [0.09501516 0.09724171 0.1000315  0.09833902 0.0969488  0.10337691\n",
      "  0.08904409 0.10779171 0.1091563  0.10305482]\n",
      " [0.09516107 0.09761645 0.09991018 0.09811526 0.09680766 0.10370047\n",
      "  0.08861842 0.10777987 0.10925557 0.10303505]\n",
      " [0.09466193 0.0975427  0.10009162 0.09824232 0.09708316 0.10356611\n",
      "  0.08890171 0.1075057  0.10919274 0.10321202]\n",
      " [0.09510786 0.09713048 0.09980202 0.09859253 0.09674562 0.10389898\n",
      "  0.08878818 0.1076386  0.108728   0.10356773]\n",
      " [0.09469669 0.09744755 0.10033939 0.09818585 0.09685549 0.10391897\n",
      "  0.0886665  0.10730584 0.10932912 0.10325459]\n",
      " [0.09471827 0.09762804 0.09993532 0.09850072 0.09706756 0.10380781\n",
      "  0.08869958 0.10765004 0.10918048 0.10281219]\n",
      " [0.09482231 0.09750335 0.09981861 0.09870929 0.09687348 0.10396069\n",
      "  0.08866028 0.10718783 0.10955042 0.10291374]\n",
      " [0.09504684 0.09729889 0.10021223 0.09855514 0.09658509 0.10347377\n",
      "  0.08868441 0.10759217 0.109166   0.10338546]\n",
      " [0.0951194  0.09718011 0.10009305 0.09873227 0.09624576 0.10381395\n",
      "  0.0886315  0.10752533 0.10928749 0.10337115]\n",
      " [0.09476462 0.09720543 0.10009704 0.0985096  0.09684318 0.10382788\n",
      "  0.08858031 0.10771971 0.10933271 0.10311953]\n",
      " [0.09476163 0.09756781 0.09967271 0.09862442 0.09660001 0.1037414\n",
      "  0.08892919 0.10748069 0.10943948 0.10318266]\n",
      " [0.09475043 0.09722961 0.10006298 0.09836988 0.09701442 0.1038313\n",
      "  0.08853826 0.10755925 0.10935018 0.1032937 ]\n",
      " [0.09482447 0.09707891 0.10009206 0.09831324 0.09709933 0.10403209\n",
      "  0.08906398 0.1075787  0.10903841 0.10287881]\n",
      " [0.09491854 0.09748611 0.10035291 0.09832572 0.09678667 0.10359663\n",
      "  0.08878436 0.10780384 0.10856627 0.10337895]\n",
      " [0.09501363 0.09734236 0.10038446 0.09839347 0.0965964  0.10416086\n",
      "  0.0887172  0.10738796 0.1091304  0.10287325]\n",
      " [0.09483391 0.09724477 0.10024716 0.09868741 0.09686895 0.10359243\n",
      "  0.08863786 0.10769819 0.1088782  0.10331112]\n",
      " [0.09496658 0.09730412 0.10018816 0.09852292 0.09678313 0.10364142\n",
      "  0.08899691 0.1075225  0.10880703 0.10326722]\n",
      " [0.09510718 0.09733414 0.10010165 0.09838123 0.09685449 0.10384226\n",
      "  0.08866565 0.10778307 0.10906067 0.10286966]\n",
      " [0.0951084  0.09738263 0.0999931  0.09864543 0.09641335 0.10345032\n",
      "  0.08894156 0.10748231 0.10918413 0.10339876]\n",
      " [0.09538559 0.09749466 0.10019251 0.09862207 0.09648196 0.10348334\n",
      "  0.08864255 0.10726818 0.10910292 0.10332622]\n",
      " [0.09443934 0.09708258 0.10015334 0.09873336 0.09718343 0.10407419\n",
      "  0.08884605 0.10725379 0.10909685 0.10313706]\n",
      " [0.09510253 0.09697363 0.10006943 0.09861795 0.09672711 0.10400243\n",
      "  0.08907324 0.10711797 0.10893861 0.1033771 ]\n",
      " [0.09481197 0.09714622 0.10014423 0.09844029 0.09679036 0.10410286\n",
      "  0.08869805 0.10761755 0.10908612 0.10316236]\n",
      " [0.09467639 0.09745315 0.10024205 0.09846929 0.09690025 0.10369137\n",
      "  0.08860791 0.10763104 0.10908472 0.10324385]\n",
      " [0.0951213  0.0974003  0.10005546 0.09874771 0.09678038 0.10372456\n",
      "  0.08848124 0.10745947 0.10909249 0.10313711]\n",
      " [0.09478496 0.09717892 0.09979007 0.09872324 0.09700915 0.10382705\n",
      "  0.0887663  0.1074762  0.10905407 0.10339005]\n",
      " [0.09491741 0.097448   0.10014233 0.09876368 0.09697313 0.10364667\n",
      "  0.08853301 0.10753477 0.10903059 0.10301041]\n",
      " [0.0945776  0.09717023 0.1001211  0.09829681 0.09691658 0.10388013\n",
      "  0.08885084 0.10752849 0.10948639 0.10317184]\n",
      " [0.09480534 0.09709417 0.09993884 0.09841186 0.09655332 0.10401717\n",
      "  0.08877118 0.10751171 0.10914854 0.10374786]\n",
      " [0.09512202 0.09723135 0.10038362 0.09843376 0.09690334 0.10379971\n",
      "  0.08855018 0.10714883 0.10930088 0.1031263 ]\n",
      " [0.09501762 0.09726533 0.1002881  0.09849631 0.09659186 0.103846\n",
      "  0.08870693 0.10730148 0.10896246 0.10352394]\n",
      " [0.09453592 0.09707858 0.1001586  0.09833209 0.0970312  0.10384395\n",
      "  0.08895281 0.10753014 0.10931138 0.10322532]]\n"
     ]
    }
   ],
   "source": [
    "# 더미 데이터를 통한 예측 \n",
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train[batch_mask]\n\u001b[0;32m     18\u001b[0m t_batch \u001b[38;5;241m=\u001b[39m t_train[batch_mask]\n\u001b[1;32m---> 20\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     23\u001b[0m     network\u001b[38;5;241m.\u001b[39mparams[key] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_late \u001b[38;5;241m*\u001b[39m grad[key]\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     41\u001b[0m loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, t)\n\u001b[0;32m     43\u001b[0m grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 44\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnumerical_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_W\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mW1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     46\u001b[0m grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\윤경록\\Desktop\\DeepLearning\\Chapter4\\common\\gradient.py:46\u001b[0m, in \u001b[0;36mnumerical_gradient\u001b[1;34m(f, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m fxh1 \u001b[38;5;241m=\u001b[39m f(x) \u001b[38;5;66;03m# f(x+h)\u001b[39;00m\n\u001b[0;32m     45\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;241m-\u001b[39m h \n\u001b[1;32m---> 46\u001b[0m fxh2 \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# f(x-h)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m grad[idx] \u001b[38;5;241m=\u001b[39m (fxh1 \u001b[38;5;241m-\u001b[39m fxh2) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mh)\n\u001b[0;32m     49\u001b[0m x[idx] \u001b[38;5;241m=\u001b[39m tmp_val \u001b[38;5;66;03m# 값 복원\u001b[39;00m\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTwoLayerNet.numerical_gradient.<locals>.<lambda>\u001b[1;34m(W)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnumerical_gradient\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 41\u001b[0m     loss_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m W: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     grads \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     44\u001b[0m     grads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m numerical_gradient(loss_W, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW1\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTwoLayerNet.loss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 28\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cross_entropy_error(y, t)\n",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36mTwoLayerNet.predict\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m b1, b2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb1\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     19\u001b[0m a1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(x, W1) \u001b[38;5;241m+\u001b[39m b1\n\u001b[1;32m---> 20\u001b[0m z1 \u001b[38;5;241m=\u001b[39m \u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m a2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(z1,W2) \u001b[38;5;241m+\u001b[39m b2\n\u001b[0;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m softmax(a2)\n",
      "File \u001b[1;32mc:\\Users\\윤경록\\Desktop\\DeepLearning\\Chapter4\\common\\functions.py:14\u001b[0m, in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msigmoid\u001b[39m(x):\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist( normalize = True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_late = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_late * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에폭 = 학습에서 훈련 데이터를 모두 소진했을 때의 횟수에 해당함. 10000개의 훈련데이터를 100개의 미니배치로 학습할 경우, SGD를 100회 반복하면 \n",
    "훈련 데이터를 모두 소진. 이경우 100회가 1에폭이 됨. 오버 피팅이 일어나는지에 대해 판단하기 위해 1에폭 당 데이터를 평가. 정확도를 기록하기로함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 개선 \n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist( normalize = True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_late = 0.1\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_late * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) +\", \" + str(test_acc))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1dc4878e534a84fda5e034b37be6921640ac5deefd0707724706874451fc7283"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
